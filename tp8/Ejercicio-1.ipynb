{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "cancer = load_breast_cancer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Clasificar los datos correspondientes al dataset de pacientes con cancer utilizando un perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de iteraciones: 27\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109)\n",
    "p = Perceptron(\n",
    "             # random_state=42,\n",
    "              max_iter=30,\n",
    "              tol=0.001,\n",
    "              #verbose = True\n",
    "              )\n",
    "p.fit(X_train, y_train)\n",
    "print(\"Cantidad de iteraciones: \" +str(p.n_iter_))\n",
    "disp = metrics.plot_confusion_matrix(p, X_test, y_test,cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Es posible clasificar correctamente los casos utilizando un Perceptron?\n",
    "\n",
    "Es posible hacer la clasificacion de los casos con Percepton \n",
    "\n",
    "- ¿Cuántas iteraciones son necesarias?\n",
    "\n",
    "Como podemos observar luego de varias ejecuciones pudimos ver la misma cantidad de iteraciones una y otra vez por eso llegamos a la conclusion de que las iteraciones necesarias son 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron multicapa\n",
    "\n",
    "Clasificar los datos correspondientes al dataset de pacientes con cancer utilizando un perceptron multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65668770\n",
      "Iteration 2, loss = 0.63787578\n",
      "Iteration 3, loss = 0.62732525\n",
      "Iteration 4, loss = 0.61997311\n",
      "Iteration 5, loss = 0.61434967\n",
      "Iteration 6, loss = 0.61162158\n",
      "Iteration 7, loss = 0.60945267\n",
      "Iteration 8, loss = 0.60726006\n",
      "Iteration 9, loss = 0.60614678\n",
      "Iteration 10, loss = 0.60586813\n",
      "Iteration 11, loss = 0.60550987\n",
      "Iteration 12, loss = 0.60459991\n",
      "Iteration 13, loss = 0.60369933\n",
      "Iteration 14, loss = 0.60352071\n",
      "Iteration 15, loss = 0.60326662\n",
      "Iteration 16, loss = 0.60243732\n",
      "Iteration 17, loss = 0.60167178\n",
      "Iteration 18, loss = 0.60137037\n",
      "Iteration 19, loss = 0.60105519\n",
      "Iteration 20, loss = 0.60081756\n",
      "Iteration 21, loss = 0.59977658\n",
      "Iteration 22, loss = 0.59935813\n",
      "Iteration 23, loss = 0.59892451\n",
      "Iteration 24, loss = 0.59847557\n",
      "Iteration 25, loss = 0.59797042\n",
      "Iteration 26, loss = 0.59751414\n",
      "Iteration 27, loss = 0.59712271\n",
      "Iteration 28, loss = 0.59669369\n",
      "Iteration 29, loss = 0.59623500\n",
      "Iteration 30, loss = 0.59584256\n",
      "Iteration 31, loss = 0.59541887\n",
      "Iteration 32, loss = 0.59499496\n",
      "Iteration 33, loss = 0.59457597\n",
      "Iteration 34, loss = 0.59420289\n",
      "Iteration 35, loss = 0.59375635\n",
      "Iteration 36, loss = 0.59342852\n",
      "Iteration 37, loss = 0.59294771\n",
      "Iteration 38, loss = 0.59279877\n",
      "Iteration 39, loss = 0.59215660\n",
      "Iteration 40, loss = 0.59174182\n",
      "Iteration 41, loss = 0.59145341\n",
      "Iteration 42, loss = 0.59105210\n",
      "Iteration 43, loss = 0.59056225\n",
      "Iteration 44, loss = 0.59024279\n",
      "Iteration 45, loss = 0.58983213\n",
      "Iteration 46, loss = 0.58939252\n",
      "Iteration 47, loss = 0.58895456\n",
      "Iteration 48, loss = 0.58860777\n",
      "Iteration 49, loss = 0.58826913\n",
      "Iteration 50, loss = 0.58784345\n",
      "Iteration 51, loss = 0.58741876\n",
      "Iteration 52, loss = 0.58732664\n",
      "Iteration 53, loss = 0.58685236\n",
      "Iteration 54, loss = 0.58636861\n",
      "Iteration 55, loss = 0.58603615\n",
      "Iteration 56, loss = 0.58558297\n",
      "Iteration 57, loss = 0.58517446\n",
      "Iteration 58, loss = 0.58511765\n",
      "Iteration 59, loss = 0.58459403\n",
      "Iteration 60, loss = 0.58402825\n",
      "Iteration 61, loss = 0.58367647\n",
      "Iteration 62, loss = 0.58344046\n",
      "Iteration 63, loss = 0.58299329\n",
      "Iteration 64, loss = 0.58259645\n",
      "Iteration 65, loss = 0.58223335\n",
      "Iteration 66, loss = 0.58189209\n",
      "Iteration 67, loss = 0.58146734\n",
      "Iteration 68, loss = 0.58112055\n",
      "Iteration 69, loss = 0.58075564\n",
      "Iteration 70, loss = 0.58042285\n",
      "Iteration 71, loss = 0.58003739\n",
      "Iteration 72, loss = 0.57970872\n",
      "Iteration 73, loss = 0.57932029\n",
      "Iteration 74, loss = 0.57894124\n",
      "Iteration 75, loss = 0.57860376\n",
      "Iteration 76, loss = 0.57826573\n",
      "Iteration 77, loss = 0.57790488\n",
      "Iteration 78, loss = 0.57733346\n",
      "Iteration 79, loss = 0.57607861\n",
      "Iteration 80, loss = 0.57207837\n",
      "Iteration 81, loss = 0.55960487\n",
      "Iteration 82, loss = 0.54128098\n",
      "Iteration 83, loss = 0.54886255\n",
      "Iteration 84, loss = 0.54380486\n",
      "Iteration 85, loss = 0.53738645\n",
      "Iteration 86, loss = 0.53908562\n",
      "Iteration 87, loss = 0.53808318\n",
      "Iteration 88, loss = 0.53589383\n",
      "Iteration 89, loss = 0.53380245\n",
      "Iteration 90, loss = 0.53319555\n",
      "Iteration 91, loss = 0.53195036\n",
      "Iteration 92, loss = 0.53029086\n",
      "Iteration 93, loss = 0.52861456\n",
      "Iteration 94, loss = 0.52811438\n",
      "Iteration 95, loss = 0.52636606\n",
      "Iteration 96, loss = 0.52513865\n",
      "Iteration 97, loss = 0.52375149\n",
      "Iteration 98, loss = 0.52265026\n",
      "Iteration 99, loss = 0.52185953\n",
      "Iteration 100, loss = 0.52053794\n",
      "Iteration 101, loss = 0.51880091\n",
      "Iteration 102, loss = 0.51765082\n",
      "Iteration 103, loss = 0.51668146\n",
      "Iteration 104, loss = 0.51573796\n",
      "Iteration 105, loss = 0.51458263\n",
      "Iteration 106, loss = 0.51334616\n",
      "Iteration 107, loss = 0.51233801\n",
      "Iteration 108, loss = 0.51121230\n",
      "Iteration 109, loss = 0.51025539\n",
      "Iteration 110, loss = 0.50924191\n",
      "Iteration 111, loss = 0.50839877\n",
      "Iteration 112, loss = 0.50804616\n",
      "Iteration 113, loss = 0.50648044\n",
      "Iteration 114, loss = 0.50532935\n",
      "Iteration 115, loss = 0.50428189\n",
      "Iteration 116, loss = 0.50361062\n",
      "Iteration 117, loss = 0.50246405\n",
      "Iteration 118, loss = 0.50153584\n",
      "Iteration 119, loss = 0.50090460\n",
      "Iteration 120, loss = 0.49934213\n",
      "Iteration 121, loss = 0.49864048\n",
      "Iteration 122, loss = 0.49787050\n",
      "Iteration 123, loss = 0.49650401\n",
      "Iteration 124, loss = 0.49563114\n",
      "Iteration 125, loss = 0.49478282\n",
      "Iteration 126, loss = 0.49388620\n",
      "Iteration 127, loss = 0.49393367\n",
      "Iteration 128, loss = 0.49224150\n",
      "Iteration 129, loss = 0.49116942\n",
      "Iteration 130, loss = 0.49083385\n",
      "Iteration 131, loss = 0.48961758\n",
      "Iteration 132, loss = 0.48869088\n",
      "Iteration 133, loss = 0.48942989\n",
      "Iteration 134, loss = 0.48702101\n",
      "Iteration 135, loss = 0.48629789\n",
      "Iteration 136, loss = 0.48545707\n",
      "Iteration 137, loss = 0.48481526\n",
      "Iteration 138, loss = 0.48397679\n",
      "Iteration 139, loss = 0.48334134\n",
      "Iteration 140, loss = 0.48244251\n",
      "Iteration 141, loss = 0.48156249\n",
      "Iteration 142, loss = 0.48086075\n",
      "Iteration 143, loss = 0.48017982\n",
      "Iteration 144, loss = 0.47925445\n",
      "Iteration 145, loss = 0.47895661\n",
      "Iteration 146, loss = 0.47785906\n",
      "Iteration 147, loss = 0.47750106\n",
      "Iteration 148, loss = 0.47656332\n",
      "Iteration 149, loss = 0.47615050\n",
      "Iteration 150, loss = 0.47541168\n",
      "Iteration 151, loss = 0.47412553\n",
      "Iteration 152, loss = 0.47369385\n",
      "Iteration 153, loss = 0.47358410\n",
      "Iteration 154, loss = 0.47208598\n",
      "Iteration 155, loss = 0.47151235\n",
      "Iteration 156, loss = 0.47080924\n",
      "Iteration 157, loss = 0.47028202\n",
      "Iteration 158, loss = 0.46941190\n",
      "Iteration 159, loss = 0.46870195\n",
      "Iteration 160, loss = 0.46803701\n",
      "Iteration 161, loss = 0.46798059\n",
      "Iteration 162, loss = 0.46676787\n",
      "Iteration 163, loss = 0.46693790\n",
      "Iteration 164, loss = 0.46656409\n",
      "Iteration 165, loss = 0.46621888\n",
      "Iteration 166, loss = 0.46525938\n",
      "Iteration 167, loss = 0.46366172\n",
      "Iteration 168, loss = 0.46339037\n",
      "Iteration 169, loss = 0.46335713\n",
      "Iteration 170, loss = 0.46157690\n",
      "Iteration 171, loss = 0.46323245\n",
      "Iteration 172, loss = 0.46053835\n",
      "Iteration 173, loss = 0.46081378\n",
      "Iteration 174, loss = 0.46027253\n",
      "Iteration 175, loss = 0.45825205\n",
      "Iteration 176, loss = 0.45829169\n",
      "Iteration 177, loss = 0.45778648\n",
      "Iteration 178, loss = 0.45664002\n",
      "Iteration 179, loss = 0.45644259\n",
      "Iteration 180, loss = 0.45548116\n",
      "Iteration 181, loss = 0.45481553\n",
      "Iteration 182, loss = 0.45434591\n",
      "Iteration 183, loss = 0.45366329\n",
      "Iteration 184, loss = 0.45335005\n",
      "Iteration 185, loss = 0.45324177\n",
      "Iteration 186, loss = 0.45229471\n",
      "Iteration 187, loss = 0.45117478\n",
      "Iteration 188, loss = 0.45130697\n",
      "Iteration 189, loss = 0.45041629\n",
      "Iteration 190, loss = 0.45110283\n",
      "Iteration 191, loss = 0.44923477\n",
      "Iteration 192, loss = 0.44819674\n",
      "Iteration 193, loss = 0.44837546\n",
      "Iteration 194, loss = 0.44743200\n",
      "Iteration 195, loss = 0.44797510\n",
      "Iteration 196, loss = 0.44730014\n",
      "Iteration 197, loss = 0.44626194\n",
      "Iteration 198, loss = 0.44503595\n",
      "Iteration 199, loss = 0.44591724\n",
      "Iteration 200, loss = 0.44421814\n",
      "Iteration 201, loss = 0.44402332\n",
      "Iteration 202, loss = 0.44300580\n",
      "Iteration 203, loss = 0.44206487\n",
      "Iteration 204, loss = 0.44239331\n",
      "Iteration 205, loss = 0.44096519\n",
      "Iteration 206, loss = 0.44051789\n",
      "Iteration 207, loss = 0.44024124\n",
      "Iteration 208, loss = 0.43954495\n",
      "Iteration 209, loss = 0.43904895\n",
      "Iteration 210, loss = 0.43879121\n",
      "Iteration 211, loss = 0.43792867\n",
      "Iteration 212, loss = 0.43803157\n",
      "Iteration 213, loss = 0.43677572\n",
      "Iteration 214, loss = 0.43614275\n",
      "Iteration 215, loss = 0.43572090\n",
      "Iteration 216, loss = 0.43500943\n",
      "Iteration 217, loss = 0.43457661\n",
      "Iteration 218, loss = 0.43424736\n",
      "Iteration 219, loss = 0.43371765\n",
      "Iteration 220, loss = 0.43305974\n",
      "Iteration 221, loss = 0.43274170\n",
      "Iteration 222, loss = 0.43227362\n",
      "Iteration 223, loss = 0.43160844\n",
      "Iteration 224, loss = 0.43086548\n",
      "Iteration 225, loss = 0.43163469\n",
      "Iteration 226, loss = 0.43000563\n",
      "Iteration 227, loss = 0.42947359\n",
      "Iteration 228, loss = 0.42892306\n",
      "Iteration 229, loss = 0.42925766\n",
      "Iteration 230, loss = 0.42797519\n",
      "Iteration 231, loss = 0.42742301\n",
      "Iteration 232, loss = 0.42673084\n",
      "Iteration 233, loss = 0.42637078\n",
      "Iteration 234, loss = 0.42588099\n",
      "Iteration 235, loss = 0.42504978\n",
      "Iteration 236, loss = 0.42484168\n",
      "Iteration 237, loss = 0.42431760\n",
      "Iteration 238, loss = 0.42481639\n",
      "Iteration 239, loss = 0.42334417\n",
      "Iteration 240, loss = 0.42270287\n",
      "Iteration 241, loss = 0.42412420\n",
      "Iteration 242, loss = 0.42182930\n",
      "Iteration 243, loss = 0.42195570\n",
      "Iteration 244, loss = 0.42097332\n",
      "Iteration 245, loss = 0.42040220\n",
      "Iteration 246, loss = 0.41974304\n",
      "Iteration 247, loss = 0.41967956\n",
      "Iteration 248, loss = 0.41918636\n",
      "Iteration 249, loss = 0.41837306\n",
      "Iteration 250, loss = 0.41843417\n",
      "Iteration 251, loss = 0.41735147\n",
      "Iteration 252, loss = 0.41818747\n",
      "Iteration 253, loss = 0.41729336\n",
      "Iteration 254, loss = 0.41699319\n",
      "Iteration 255, loss = 0.41576954\n",
      "Iteration 256, loss = 0.41693260\n",
      "Iteration 257, loss = 0.41547318\n",
      "Iteration 258, loss = 0.41470727\n",
      "Iteration 259, loss = 0.41427348\n",
      "Iteration 260, loss = 0.41320833\n",
      "Iteration 261, loss = 0.41418997\n",
      "Iteration 262, loss = 0.41421458\n",
      "Iteration 263, loss = 0.41246855\n",
      "Iteration 264, loss = 0.41179466\n",
      "Iteration 265, loss = 0.41112179\n",
      "Iteration 266, loss = 0.41103482\n",
      "Iteration 267, loss = 0.41082041\n",
      "Iteration 268, loss = 0.40936175\n",
      "Iteration 269, loss = 0.40969167\n",
      "Iteration 270, loss = 0.40928924\n",
      "Iteration 271, loss = 0.40878468\n",
      "Iteration 272, loss = 0.40818964\n",
      "Iteration 273, loss = 0.40781843\n",
      "Iteration 274, loss = 0.40718922\n",
      "Iteration 275, loss = 0.40665174\n",
      "Iteration 276, loss = 0.40660350\n",
      "Iteration 277, loss = 0.40563699\n",
      "Iteration 278, loss = 0.40496000\n",
      "Iteration 279, loss = 0.40530223\n",
      "Iteration 280, loss = 0.40438111\n",
      "Iteration 281, loss = 0.40363149\n",
      "Iteration 282, loss = 0.40370999\n",
      "Iteration 283, loss = 0.40315509\n",
      "Iteration 284, loss = 0.40243810\n",
      "Iteration 285, loss = 0.40213233\n",
      "Iteration 286, loss = 0.40246058\n",
      "Iteration 287, loss = 0.40138071\n",
      "Iteration 288, loss = 0.40074433\n",
      "Iteration 289, loss = 0.40032534\n",
      "Iteration 290, loss = 0.39999502\n",
      "Iteration 291, loss = 0.39986666\n",
      "Iteration 292, loss = 0.39936129\n",
      "Iteration 293, loss = 0.39853810\n",
      "Iteration 294, loss = 0.39832951\n",
      "Iteration 295, loss = 0.39777502\n",
      "Iteration 296, loss = 0.39739260\n",
      "Iteration 297, loss = 0.39716415\n",
      "Iteration 298, loss = 0.39736998\n",
      "Iteration 299, loss = 0.39614541\n",
      "Iteration 300, loss = 0.39566064\n",
      "Iteration 301, loss = 0.39520663\n",
      "Iteration 302, loss = 0.39524361\n",
      "Iteration 303, loss = 0.39466492\n",
      "Iteration 304, loss = 0.39429136\n",
      "Iteration 305, loss = 0.39347416\n",
      "Iteration 306, loss = 0.39314878\n",
      "Iteration 307, loss = 0.39270328\n",
      "Iteration 308, loss = 0.39225439\n",
      "Iteration 309, loss = 0.39195114\n",
      "Iteration 310, loss = 0.39142230\n",
      "Iteration 311, loss = 0.39126452\n",
      "Iteration 312, loss = 0.39059495\n",
      "Iteration 313, loss = 0.39030629\n",
      "Iteration 314, loss = 0.38983677\n",
      "Iteration 315, loss = 0.38975741\n",
      "Iteration 316, loss = 0.38895664\n",
      "Iteration 317, loss = 0.38905216\n",
      "Iteration 318, loss = 0.38857414\n",
      "Iteration 319, loss = 0.38786344\n",
      "Iteration 320, loss = 0.38746270\n",
      "Iteration 321, loss = 0.38699686\n",
      "Iteration 322, loss = 0.38659014\n",
      "Iteration 323, loss = 0.38633627\n",
      "Iteration 324, loss = 0.38607786\n",
      "Iteration 325, loss = 0.38541587\n",
      "Iteration 326, loss = 0.38495706\n",
      "Iteration 327, loss = 0.38469958\n",
      "Iteration 328, loss = 0.38500767\n",
      "Iteration 329, loss = 0.38548652\n",
      "Iteration 330, loss = 0.38421700\n",
      "Iteration 331, loss = 0.38368376\n",
      "Iteration 332, loss = 0.38285632\n",
      "Iteration 333, loss = 0.38228081\n",
      "Iteration 334, loss = 0.38213419\n",
      "Iteration 335, loss = 0.38163860\n",
      "Iteration 336, loss = 0.38107645\n",
      "Iteration 337, loss = 0.38083072\n",
      "Iteration 338, loss = 0.38040174\n",
      "Iteration 339, loss = 0.37995032\n",
      "Iteration 340, loss = 0.37955433\n",
      "Iteration 341, loss = 0.37914389\n",
      "Iteration 342, loss = 0.37878381\n",
      "Iteration 343, loss = 0.37850726\n",
      "Iteration 344, loss = 0.37804061\n",
      "Iteration 345, loss = 0.37905256\n",
      "Iteration 346, loss = 0.37743397\n",
      "Iteration 347, loss = 0.37703671\n",
      "Iteration 348, loss = 0.37669878\n",
      "Iteration 349, loss = 0.37623273\n",
      "Iteration 350, loss = 0.37605937\n",
      "Iteration 351, loss = 0.37549114\n",
      "Iteration 352, loss = 0.37556711\n",
      "Iteration 353, loss = 0.37535250\n",
      "Iteration 354, loss = 0.37476999\n",
      "Iteration 355, loss = 0.37394407\n",
      "Iteration 356, loss = 0.37454951\n",
      "Iteration 357, loss = 0.37513516\n",
      "Iteration 358, loss = 0.37342064\n",
      "Iteration 359, loss = 0.37232420\n",
      "Iteration 360, loss = 0.37273834\n",
      "Iteration 361, loss = 0.37446483\n",
      "Iteration 362, loss = 0.37326350\n",
      "Iteration 363, loss = 0.37137457\n",
      "Iteration 364, loss = 0.37162145\n",
      "Iteration 365, loss = 0.37060204\n",
      "Iteration 366, loss = 0.37040003\n",
      "Iteration 367, loss = 0.37021793\n",
      "Iteration 368, loss = 0.36979616\n",
      "Iteration 369, loss = 0.36993052\n",
      "Iteration 370, loss = 0.36835409\n",
      "Iteration 371, loss = 0.36850336\n",
      "Iteration 372, loss = 0.36766389\n",
      "Iteration 373, loss = 0.36731693\n",
      "Iteration 374, loss = 0.36899325\n",
      "Iteration 375, loss = 0.36680021\n",
      "Iteration 376, loss = 0.36682088\n",
      "Iteration 377, loss = 0.36753020\n",
      "Iteration 378, loss = 0.36743534\n",
      "Iteration 379, loss = 0.36665002\n",
      "Iteration 380, loss = 0.36657731\n",
      "Iteration 381, loss = 0.36493138\n",
      "Iteration 382, loss = 0.36459227\n",
      "Iteration 383, loss = 0.36392683\n",
      "Iteration 384, loss = 0.36337970\n",
      "Iteration 385, loss = 0.36406030\n",
      "Iteration 386, loss = 0.36264226\n",
      "Iteration 387, loss = 0.36374978\n",
      "Iteration 388, loss = 0.36290476\n",
      "Iteration 389, loss = 0.36178181\n",
      "Iteration 390, loss = 0.36166467\n",
      "Iteration 391, loss = 0.36126140\n",
      "Iteration 392, loss = 0.36078749\n",
      "Iteration 393, loss = 0.36066548\n",
      "Iteration 394, loss = 0.36013756\n",
      "Iteration 395, loss = 0.36148483\n",
      "Iteration 396, loss = 0.35954420\n",
      "Iteration 397, loss = 0.36038753\n",
      "Iteration 398, loss = 0.35948773\n",
      "Iteration 399, loss = 0.35853781\n",
      "Iteration 400, loss = 0.35814817\n",
      "Iteration 401, loss = 0.35857709\n",
      "Iteration 402, loss = 0.35700219\n",
      "Iteration 403, loss = 0.35692374\n",
      "Iteration 404, loss = 0.35757288\n",
      "Iteration 405, loss = 0.35607794\n",
      "Iteration 406, loss = 0.35916499\n",
      "Iteration 407, loss = 0.35642082\n",
      "Iteration 408, loss = 0.35708170\n",
      "Iteration 409, loss = 0.35617259\n",
      "Iteration 410, loss = 0.35489924\n",
      "Iteration 411, loss = 0.35520352\n",
      "Iteration 412, loss = 0.35372433\n",
      "Iteration 413, loss = 0.35576250\n",
      "Iteration 414, loss = 0.35538523\n",
      "Iteration 415, loss = 0.35290711\n",
      "Iteration 416, loss = 0.35283923\n",
      "Iteration 417, loss = 0.35355709\n",
      "Iteration 418, loss = 0.35245391\n",
      "Iteration 419, loss = 0.35142020\n",
      "Iteration 420, loss = 0.35295201\n",
      "Iteration 421, loss = 0.35141803\n",
      "Iteration 422, loss = 0.35067674\n",
      "Iteration 423, loss = 0.35142901\n",
      "Iteration 424, loss = 0.35029543\n",
      "Iteration 425, loss = 0.34974504\n",
      "Iteration 426, loss = 0.35111967\n",
      "Iteration 427, loss = 0.34916467\n",
      "Iteration 428, loss = 0.34882873\n",
      "Iteration 429, loss = 0.34848957\n",
      "Iteration 430, loss = 0.34951479\n",
      "Iteration 431, loss = 0.34759435\n",
      "Iteration 432, loss = 0.34740730\n",
      "Iteration 433, loss = 0.34716308\n",
      "Iteration 434, loss = 0.34748446\n",
      "Iteration 435, loss = 0.34637794\n",
      "Iteration 436, loss = 0.34613880\n",
      "Iteration 437, loss = 0.34594694\n",
      "Iteration 438, loss = 0.34535362\n",
      "Iteration 439, loss = 0.34507847\n",
      "Iteration 440, loss = 0.34530163\n",
      "Iteration 441, loss = 0.34496283\n",
      "Iteration 442, loss = 0.34473823\n",
      "Iteration 443, loss = 0.34495427\n",
      "Iteration 444, loss = 0.34340471\n",
      "Iteration 445, loss = 0.34360868\n",
      "Iteration 446, loss = 0.34312254\n",
      "Iteration 447, loss = 0.34237199\n",
      "Iteration 448, loss = 0.34255581\n",
      "Iteration 449, loss = 0.34172131\n",
      "Iteration 450, loss = 0.34138127\n",
      "Iteration 451, loss = 0.34142318\n",
      "Iteration 452, loss = 0.34083378\n",
      "Iteration 453, loss = 0.34078759\n",
      "Iteration 454, loss = 0.34037577\n",
      "Iteration 455, loss = 0.34013999\n",
      "Iteration 456, loss = 0.33954579\n",
      "Iteration 457, loss = 0.33923296\n",
      "Iteration 458, loss = 0.33905009\n",
      "Iteration 459, loss = 0.33864461\n",
      "Iteration 460, loss = 0.33827215\n",
      "Iteration 461, loss = 0.33814734\n",
      "Iteration 462, loss = 0.33777037\n",
      "Iteration 463, loss = 0.33858178\n",
      "Iteration 464, loss = 0.33752143\n",
      "Iteration 465, loss = 0.33851791\n",
      "Iteration 466, loss = 0.33664927\n",
      "Iteration 467, loss = 0.33643808\n",
      "Iteration 468, loss = 0.33582501\n",
      "Iteration 469, loss = 0.33553859\n",
      "Iteration 470, loss = 0.33534506\n",
      "Iteration 471, loss = 0.33490326\n",
      "Iteration 472, loss = 0.33471831\n",
      "Iteration 473, loss = 0.33450568\n",
      "Iteration 474, loss = 0.33399094\n",
      "Iteration 475, loss = 0.33411552\n",
      "Iteration 476, loss = 0.33379393\n",
      "Iteration 477, loss = 0.33354204\n",
      "Iteration 478, loss = 0.33354438\n",
      "Iteration 479, loss = 0.33238050\n",
      "Iteration 480, loss = 0.33235200\n",
      "Iteration 481, loss = 0.33221047\n",
      "Iteration 482, loss = 0.33153096\n",
      "Iteration 483, loss = 0.33228140\n",
      "Iteration 484, loss = 0.33243803\n",
      "Iteration 485, loss = 0.33029308\n",
      "Iteration 486, loss = 0.33190862\n",
      "Iteration 487, loss = 0.33590749\n",
      "Iteration 488, loss = 0.33137624\n",
      "Iteration 489, loss = 0.32957952\n",
      "Iteration 490, loss = 0.33073435\n",
      "Iteration 491, loss = 0.32945007\n",
      "Iteration 492, loss = 0.33068200\n",
      "Iteration 493, loss = 0.33301884\n",
      "Iteration 494, loss = 0.32808897\n",
      "Iteration 495, loss = 0.32805130\n",
      "Iteration 496, loss = 0.32725541\n",
      "Iteration 497, loss = 0.32719089\n",
      "Iteration 498, loss = 0.32706985\n",
      "Iteration 499, loss = 0.32663719\n",
      "Iteration 500, loss = 0.32646499\n",
      "Iteration 501, loss = 0.32684178\n",
      "Iteration 502, loss = 0.32574062\n",
      "Iteration 503, loss = 0.32734039\n",
      "Iteration 504, loss = 0.32536138\n",
      "Iteration 505, loss = 0.32521006\n",
      "Iteration 506, loss = 0.32570114\n",
      "Iteration 507, loss = 0.32428355\n",
      "Iteration 508, loss = 0.32526605\n",
      "Iteration 509, loss = 0.32389620\n",
      "Iteration 510, loss = 0.32544489\n",
      "Iteration 511, loss = 0.32351608\n",
      "Iteration 512, loss = 0.32258376\n",
      "Iteration 513, loss = 0.32294264\n",
      "Iteration 514, loss = 0.32219075\n",
      "Iteration 515, loss = 0.32194011\n",
      "Iteration 516, loss = 0.32154488\n",
      "Iteration 517, loss = 0.32123571\n",
      "Iteration 518, loss = 0.32091665\n",
      "Iteration 519, loss = 0.32079108\n",
      "Iteration 520, loss = 0.32032912\n",
      "Iteration 521, loss = 0.32060147\n",
      "Iteration 522, loss = 0.31998006\n",
      "Iteration 523, loss = 0.32002517\n",
      "Iteration 524, loss = 0.31922809\n",
      "Iteration 525, loss = 0.31893097\n",
      "Iteration 526, loss = 0.31945975\n",
      "Iteration 527, loss = 0.31831207\n",
      "Iteration 528, loss = 0.31818219\n",
      "Iteration 529, loss = 0.31941892\n",
      "Iteration 530, loss = 0.31755416\n",
      "Iteration 531, loss = 0.31769836\n",
      "Iteration 532, loss = 0.31776893\n",
      "Iteration 533, loss = 0.31795730\n",
      "Iteration 534, loss = 0.31689651\n",
      "Iteration 535, loss = 0.31731051\n",
      "Iteration 536, loss = 0.31575975\n",
      "Iteration 537, loss = 0.31666752\n",
      "Iteration 538, loss = 0.31579737\n",
      "Iteration 539, loss = 0.31573391\n",
      "Iteration 540, loss = 0.31673932\n",
      "Iteration 541, loss = 0.31455189\n",
      "Iteration 542, loss = 0.31406701\n",
      "Iteration 543, loss = 0.31422553\n",
      "Iteration 544, loss = 0.31380518\n",
      "Iteration 545, loss = 0.31313075\n",
      "Iteration 546, loss = 0.31479917\n",
      "Iteration 547, loss = 0.31459164\n",
      "Iteration 548, loss = 0.31299965\n",
      "Iteration 549, loss = 0.31222409\n",
      "Iteration 550, loss = 0.31227735\n",
      "Iteration 551, loss = 0.31186708\n",
      "Iteration 552, loss = 0.31134917\n",
      "Iteration 553, loss = 0.31449737\n",
      "Iteration 554, loss = 0.31103879\n",
      "Iteration 555, loss = 0.31126383\n",
      "Iteration 556, loss = 0.31116066\n",
      "Iteration 557, loss = 0.31048137\n",
      "Iteration 558, loss = 0.31160778\n",
      "Iteration 559, loss = 0.31085513\n",
      "Iteration 560, loss = 0.31245075\n",
      "Iteration 561, loss = 0.31116758\n",
      "Iteration 562, loss = 0.31109433\n",
      "Iteration 563, loss = 0.30935200\n",
      "Iteration 564, loss = 0.30994896\n",
      "Iteration 565, loss = 0.30917996\n",
      "Iteration 566, loss = 0.30887082\n",
      "Iteration 567, loss = 0.30870247\n",
      "Iteration 568, loss = 0.30907807\n",
      "Iteration 569, loss = 0.30859824\n",
      "Iteration 570, loss = 0.30702232\n",
      "Iteration 571, loss = 0.30723601\n",
      "Iteration 572, loss = 0.30634196\n",
      "Iteration 573, loss = 0.30606252\n",
      "Iteration 574, loss = 0.30553923\n",
      "Iteration 575, loss = 0.30499192\n",
      "Iteration 576, loss = 0.30532308\n",
      "Iteration 577, loss = 0.30467037\n",
      "Iteration 578, loss = 0.30531149\n",
      "Iteration 579, loss = 0.30591140\n",
      "Iteration 580, loss = 0.30451070\n",
      "Iteration 581, loss = 0.30371633\n",
      "Iteration 582, loss = 0.30411313\n",
      "Iteration 583, loss = 0.30409681\n",
      "Iteration 584, loss = 0.30602803\n",
      "Iteration 585, loss = 0.30231211\n",
      "Iteration 586, loss = 0.30376274\n",
      "Iteration 587, loss = 0.30323096\n",
      "Iteration 588, loss = 0.30179608\n",
      "Iteration 589, loss = 0.30194265\n",
      "Iteration 590, loss = 0.30144791\n",
      "Iteration 591, loss = 0.30125394\n",
      "Iteration 592, loss = 0.30077865\n",
      "Iteration 593, loss = 0.30180818\n",
      "Iteration 594, loss = 0.30061315\n",
      "Iteration 595, loss = 0.30287201\n",
      "Iteration 596, loss = 0.29989292\n",
      "Iteration 597, loss = 0.30032690\n",
      "Iteration 598, loss = 0.30042648\n",
      "Iteration 599, loss = 0.29818992\n",
      "Iteration 600, loss = 0.29958113\n",
      "Iteration 601, loss = 0.30050813\n",
      "Iteration 602, loss = 0.29862045\n",
      "Iteration 603, loss = 0.29781216\n",
      "Iteration 604, loss = 0.29804412\n",
      "Iteration 605, loss = 0.29733931\n",
      "Iteration 606, loss = 0.29755867\n",
      "Iteration 607, loss = 0.29759205\n",
      "Iteration 608, loss = 0.29717106\n",
      "Iteration 609, loss = 0.29623157\n",
      "Iteration 610, loss = 0.29609129\n",
      "Iteration 611, loss = 0.29700871\n",
      "Iteration 612, loss = 0.29568611\n",
      "Iteration 613, loss = 0.29658865\n",
      "Iteration 614, loss = 0.29497155\n",
      "Iteration 615, loss = 0.29458743\n",
      "Iteration 616, loss = 0.29641339\n",
      "Iteration 617, loss = 0.29619896\n",
      "Iteration 618, loss = 0.29454924\n",
      "Iteration 619, loss = 0.29338299\n",
      "Iteration 620, loss = 0.29406111\n",
      "Iteration 621, loss = 0.29461048\n",
      "Iteration 622, loss = 0.29273855\n",
      "Iteration 623, loss = 0.29707039\n",
      "Iteration 624, loss = 0.29294047\n",
      "Iteration 625, loss = 0.29302962\n",
      "Iteration 626, loss = 0.29421756\n",
      "Iteration 627, loss = 0.29180953\n",
      "Iteration 628, loss = 0.29164808\n",
      "Iteration 629, loss = 0.29212677\n",
      "Iteration 630, loss = 0.29232917\n",
      "Iteration 631, loss = 0.29060818\n",
      "Iteration 632, loss = 0.29148051\n",
      "Iteration 633, loss = 0.29039000\n",
      "Iteration 634, loss = 0.29006872\n",
      "Iteration 635, loss = 0.29062090\n",
      "Iteration 636, loss = 0.28941444\n",
      "Iteration 637, loss = 0.29242803\n",
      "Iteration 638, loss = 0.28923921\n",
      "Iteration 639, loss = 0.28928487\n",
      "Iteration 640, loss = 0.28973572\n",
      "Iteration 641, loss = 0.28872100\n",
      "Iteration 642, loss = 0.28863109\n",
      "Iteration 643, loss = 0.29004041\n",
      "Iteration 644, loss = 0.28732575\n",
      "Iteration 645, loss = 0.28866418\n",
      "Iteration 646, loss = 0.28673858\n",
      "Iteration 647, loss = 0.28898867\n",
      "Iteration 648, loss = 0.28789617\n",
      "Iteration 649, loss = 0.28691160\n",
      "Iteration 650, loss = 0.28845780\n",
      "Iteration 651, loss = 0.28533566\n",
      "Iteration 652, loss = 0.28699273\n",
      "Iteration 653, loss = 0.28647864\n",
      "Iteration 654, loss = 0.28647321\n",
      "Iteration 655, loss = 0.28657830\n",
      "Iteration 656, loss = 0.28436536\n",
      "Iteration 657, loss = 0.28601502\n",
      "Iteration 658, loss = 0.28621463\n",
      "Iteration 659, loss = 0.28398511\n",
      "Iteration 660, loss = 0.28436135\n",
      "Iteration 661, loss = 0.28378550\n",
      "Iteration 662, loss = 0.28307121\n",
      "Iteration 663, loss = 0.28383230\n",
      "Iteration 664, loss = 0.28235563\n",
      "Iteration 665, loss = 0.28247458\n",
      "Iteration 666, loss = 0.28259045\n",
      "Iteration 667, loss = 0.28306310\n",
      "Iteration 668, loss = 0.28248615\n",
      "Iteration 669, loss = 0.28464322\n",
      "Iteration 670, loss = 0.28259323\n",
      "Iteration 671, loss = 0.28119766\n",
      "Iteration 672, loss = 0.28126483\n",
      "Iteration 673, loss = 0.28090597\n",
      "Iteration 674, loss = 0.28080672\n",
      "Iteration 675, loss = 0.28016161\n",
      "Iteration 676, loss = 0.27961954\n",
      "Iteration 677, loss = 0.28128760\n",
      "Iteration 678, loss = 0.27956220\n",
      "Iteration 679, loss = 0.28062197\n",
      "Iteration 680, loss = 0.27917309\n",
      "Iteration 681, loss = 0.28219810\n",
      "Iteration 682, loss = 0.28111428\n",
      "Iteration 683, loss = 0.27854334\n",
      "Iteration 684, loss = 0.27877101\n",
      "Iteration 685, loss = 0.27856820\n",
      "Iteration 686, loss = 0.27760641\n",
      "Iteration 687, loss = 0.27741622\n",
      "Iteration 688, loss = 0.27674022\n",
      "Iteration 689, loss = 0.27749031\n",
      "Iteration 690, loss = 0.27796547\n",
      "Iteration 691, loss = 0.27690148\n",
      "Iteration 692, loss = 0.27581435\n",
      "Iteration 693, loss = 0.27598676\n",
      "Iteration 694, loss = 0.27547561\n",
      "Iteration 695, loss = 0.27512642\n",
      "Iteration 696, loss = 0.27492664\n",
      "Iteration 697, loss = 0.27478583\n",
      "Iteration 698, loss = 0.27445689\n",
      "Iteration 699, loss = 0.27429094\n",
      "Iteration 700, loss = 0.27424396\n",
      "Iteration 701, loss = 0.27416417\n",
      "Iteration 702, loss = 0.27346403\n",
      "Iteration 703, loss = 0.27352609\n",
      "Iteration 704, loss = 0.27299168\n",
      "Iteration 705, loss = 0.27293533\n",
      "Iteration 706, loss = 0.27365888\n",
      "Iteration 707, loss = 0.27369389\n",
      "Iteration 708, loss = 0.27251574\n",
      "Iteration 709, loss = 0.27178935\n",
      "Iteration 710, loss = 0.27250898\n",
      "Iteration 711, loss = 0.27219574\n",
      "Iteration 712, loss = 0.27118645\n",
      "Iteration 713, loss = 0.27170825\n",
      "Iteration 714, loss = 0.27092211\n",
      "Iteration 715, loss = 0.27106159\n",
      "Iteration 716, loss = 0.27150630\n",
      "Iteration 717, loss = 0.26991741\n",
      "Iteration 718, loss = 0.27127335\n",
      "Iteration 719, loss = 0.27086592\n",
      "Iteration 720, loss = 0.27029256\n",
      "Iteration 721, loss = 0.26935249\n",
      "Iteration 722, loss = 0.26918631\n",
      "Iteration 723, loss = 0.26880551\n",
      "Iteration 724, loss = 0.26835945\n",
      "Iteration 725, loss = 0.26979786\n",
      "Iteration 726, loss = 0.26811678\n",
      "Iteration 727, loss = 0.26861778\n",
      "Iteration 728, loss = 0.26755017\n",
      "Iteration 729, loss = 0.26818396\n",
      "Iteration 730, loss = 0.26709548\n",
      "Iteration 731, loss = 0.26693917\n",
      "Iteration 732, loss = 0.26683586\n",
      "Iteration 733, loss = 0.26655013\n",
      "Iteration 734, loss = 0.26680081\n",
      "Iteration 735, loss = 0.26745633\n",
      "Iteration 736, loss = 0.26604447\n",
      "Iteration 737, loss = 0.26651001\n",
      "Iteration 738, loss = 0.26690350\n",
      "Iteration 739, loss = 0.26510414\n",
      "Iteration 740, loss = 0.26696671\n",
      "Iteration 741, loss = 0.26702864\n",
      "Iteration 742, loss = 0.26580165\n",
      "Iteration 743, loss = 0.26431218\n",
      "Iteration 744, loss = 0.26458713\n",
      "Iteration 745, loss = 0.26465539\n",
      "Iteration 746, loss = 0.26347284\n",
      "Iteration 747, loss = 0.26332899\n",
      "Iteration 748, loss = 0.26332885\n",
      "Iteration 749, loss = 0.26290826\n",
      "Iteration 750, loss = 0.26299284\n",
      "Iteration 751, loss = 0.26292514\n",
      "Iteration 752, loss = 0.26235747\n",
      "Iteration 753, loss = 0.26235639\n",
      "Iteration 754, loss = 0.26273843\n",
      "Iteration 755, loss = 0.26316003\n",
      "Iteration 756, loss = 0.26262843\n",
      "Iteration 757, loss = 0.26153704\n",
      "Iteration 758, loss = 0.26328184\n",
      "Iteration 759, loss = 0.26489023\n",
      "Iteration 760, loss = 0.26365399\n",
      "Iteration 761, loss = 0.26143721\n",
      "Iteration 762, loss = 0.26465314\n",
      "Iteration 763, loss = 0.26035500\n",
      "Iteration 764, loss = 0.26007916\n",
      "Iteration 765, loss = 0.25973285\n",
      "Iteration 766, loss = 0.26039149\n",
      "Iteration 767, loss = 0.25976420\n",
      "Iteration 768, loss = 0.25904319\n",
      "Iteration 769, loss = 0.25955509\n",
      "Iteration 770, loss = 0.25834303\n",
      "Iteration 771, loss = 0.25904654\n",
      "Iteration 772, loss = 0.25822989\n",
      "Iteration 773, loss = 0.25792773\n",
      "Iteration 774, loss = 0.25917748\n",
      "Iteration 775, loss = 0.25915055\n",
      "Iteration 776, loss = 0.25854394\n",
      "Iteration 777, loss = 0.25841205\n",
      "Iteration 778, loss = 0.25821260\n",
      "Iteration 779, loss = 0.25735367\n",
      "Iteration 780, loss = 0.25606249\n",
      "Iteration 781, loss = 0.25770517\n",
      "Iteration 782, loss = 0.25611676\n",
      "Iteration 783, loss = 0.26007968\n",
      "Iteration 784, loss = 0.25597516\n",
      "Iteration 785, loss = 0.25650714\n",
      "Iteration 786, loss = 0.25706933\n",
      "Iteration 787, loss = 0.25462303\n",
      "Iteration 788, loss = 0.25557952\n",
      "Iteration 789, loss = 0.25869836\n",
      "Iteration 790, loss = 0.25463444\n",
      "Iteration 791, loss = 0.25381981\n",
      "Iteration 792, loss = 0.25473674\n",
      "Iteration 793, loss = 0.25360785\n",
      "Iteration 794, loss = 0.25555947\n",
      "Iteration 795, loss = 0.25476752\n",
      "Iteration 796, loss = 0.25502153\n",
      "Iteration 797, loss = 0.25309062\n",
      "Iteration 798, loss = 0.25368049\n",
      "Iteration 799, loss = 0.25402076\n",
      "Iteration 800, loss = 0.25217258\n",
      "Iteration 801, loss = 0.25459489\n",
      "Iteration 802, loss = 0.25202031\n",
      "Iteration 803, loss = 0.25664991\n",
      "Iteration 804, loss = 0.25297292\n",
      "Iteration 805, loss = 0.25219677\n",
      "Iteration 806, loss = 0.25267780\n",
      "Iteration 807, loss = 0.25262274\n",
      "Iteration 808, loss = 0.25239254\n",
      "Iteration 809, loss = 0.25299487\n",
      "Iteration 810, loss = 0.25163769\n",
      "Iteration 811, loss = 0.25069926\n",
      "Iteration 812, loss = 0.25191484\n",
      "Iteration 813, loss = 0.25048905\n",
      "Iteration 814, loss = 0.24925990\n",
      "Iteration 815, loss = 0.25316132\n",
      "Iteration 816, loss = 0.24907554\n",
      "Iteration 817, loss = 0.25213432\n",
      "Iteration 818, loss = 0.25053015\n",
      "Iteration 819, loss = 0.25350013\n",
      "Iteration 820, loss = 0.25209277\n",
      "Iteration 821, loss = 0.25117008\n",
      "Iteration 822, loss = 0.25125568\n",
      "Iteration 823, loss = 0.24699668\n",
      "Iteration 824, loss = 0.24909504\n",
      "Iteration 825, loss = 0.24857197\n",
      "Iteration 826, loss = 0.24775766\n",
      "Iteration 827, loss = 0.24672989\n",
      "Iteration 828, loss = 0.24878376\n",
      "Iteration 829, loss = 0.24824421\n",
      "Iteration 830, loss = 0.24658389\n",
      "Iteration 831, loss = 0.24605555\n",
      "Iteration 832, loss = 0.24599275\n",
      "Iteration 833, loss = 0.24619331\n",
      "Iteration 834, loss = 0.24542875\n",
      "Iteration 835, loss = 0.24981175\n",
      "Iteration 836, loss = 0.24560563\n",
      "Iteration 837, loss = 0.24685072\n",
      "Iteration 838, loss = 0.24607972\n",
      "Iteration 839, loss = 0.24904690\n",
      "Iteration 840, loss = 0.24670501\n",
      "Iteration 841, loss = 0.24407459\n",
      "Iteration 842, loss = 0.24592736\n",
      "Iteration 843, loss = 0.24322727\n",
      "Iteration 844, loss = 0.24479771\n",
      "Iteration 845, loss = 0.24478010\n",
      "Iteration 846, loss = 0.24317530\n",
      "Iteration 847, loss = 0.24351568\n",
      "Iteration 848, loss = 0.24452278\n",
      "Iteration 849, loss = 0.24349800\n",
      "Iteration 850, loss = 0.24264422\n",
      "Iteration 851, loss = 0.24337753\n",
      "Iteration 852, loss = 0.24223997\n",
      "Iteration 853, loss = 0.24187264\n",
      "Iteration 854, loss = 0.24248905\n",
      "Iteration 855, loss = 0.24136631\n",
      "Iteration 856, loss = 0.24385358\n",
      "Iteration 857, loss = 0.24148856\n",
      "Iteration 858, loss = 0.24098871\n",
      "Iteration 859, loss = 0.24124005\n",
      "Iteration 860, loss = 0.24116215\n",
      "Iteration 861, loss = 0.24088168\n",
      "Iteration 862, loss = 0.24077094\n",
      "Iteration 863, loss = 0.23996345\n",
      "Iteration 864, loss = 0.24041814\n",
      "Iteration 865, loss = 0.24021030\n",
      "Iteration 866, loss = 0.24163936\n",
      "Iteration 867, loss = 0.24007540\n",
      "Iteration 868, loss = 0.23890222\n",
      "Iteration 869, loss = 0.23912232\n",
      "Iteration 870, loss = 0.23921643\n",
      "Iteration 871, loss = 0.23830241\n",
      "Iteration 872, loss = 0.23945771\n",
      "Iteration 873, loss = 0.23790237\n",
      "Iteration 874, loss = 0.23868601\n",
      "Iteration 875, loss = 0.23898197\n",
      "Iteration 876, loss = 0.23693983\n",
      "Iteration 877, loss = 0.23913350\n",
      "Iteration 878, loss = 0.23748086\n",
      "Iteration 879, loss = 0.23708534\n",
      "Iteration 880, loss = 0.23815662\n",
      "Iteration 881, loss = 0.23660948\n",
      "Iteration 882, loss = 0.23645108\n",
      "Iteration 883, loss = 0.23722619\n",
      "Iteration 884, loss = 0.23586796\n",
      "Iteration 885, loss = 0.23700204\n",
      "Iteration 886, loss = 0.23552014\n",
      "Iteration 887, loss = 0.23590203\n",
      "Iteration 888, loss = 0.23546367\n",
      "Iteration 889, loss = 0.23549610\n",
      "Iteration 890, loss = 0.23614722\n",
      "Iteration 891, loss = 0.23475499\n",
      "Iteration 892, loss = 0.23458077\n",
      "Iteration 893, loss = 0.23430687\n",
      "Iteration 894, loss = 0.23420347\n",
      "Iteration 895, loss = 0.23444645\n",
      "Iteration 896, loss = 0.23380971\n",
      "Iteration 897, loss = 0.23586972\n",
      "Iteration 898, loss = 0.23348972\n",
      "Iteration 899, loss = 0.23441858\n",
      "Iteration 900, loss = 0.23504473\n",
      "Iteration 901, loss = 0.23529868\n",
      "Iteration 902, loss = 0.23486081\n",
      "Iteration 903, loss = 0.23311512\n",
      "Iteration 904, loss = 0.23544379\n",
      "Iteration 905, loss = 0.23519122\n",
      "Iteration 906, loss = 0.23537205\n",
      "Iteration 907, loss = 0.23243031\n",
      "Iteration 908, loss = 0.23206664\n",
      "Iteration 909, loss = 0.23152298\n",
      "Iteration 910, loss = 0.23285257\n",
      "Iteration 911, loss = 0.23142579\n",
      "Iteration 912, loss = 0.23075246\n",
      "Iteration 913, loss = 0.23115802\n",
      "Iteration 914, loss = 0.23137148\n",
      "Iteration 915, loss = 0.23077424\n",
      "Iteration 916, loss = 0.23023982\n",
      "Iteration 917, loss = 0.23005358\n",
      "Iteration 918, loss = 0.22986294\n",
      "Iteration 919, loss = 0.23005302\n",
      "Iteration 920, loss = 0.23020958\n",
      "Iteration 921, loss = 0.22993357\n",
      "Iteration 922, loss = 0.22905261\n",
      "Iteration 923, loss = 0.22909688\n",
      "Iteration 924, loss = 0.22890227\n",
      "Iteration 925, loss = 0.22869954\n",
      "Iteration 926, loss = 0.22853187\n",
      "Iteration 927, loss = 0.22878063\n",
      "Iteration 928, loss = 0.22853021\n",
      "Iteration 929, loss = 0.22832360\n",
      "Iteration 930, loss = 0.22856082\n",
      "Iteration 931, loss = 0.22869690\n",
      "Iteration 932, loss = 0.22794287\n",
      "Iteration 933, loss = 0.22724824\n",
      "Iteration 934, loss = 0.22818422\n",
      "Iteration 935, loss = 0.22650152\n",
      "Iteration 936, loss = 0.22769288\n",
      "Iteration 937, loss = 0.22662675\n",
      "Iteration 938, loss = 0.22666451\n",
      "Iteration 939, loss = 0.22650696\n",
      "Iteration 940, loss = 0.22683819\n",
      "Iteration 941, loss = 0.22611120\n",
      "Iteration 942, loss = 0.22554547\n",
      "Iteration 943, loss = 0.22660953\n",
      "Iteration 944, loss = 0.22552909\n",
      "Iteration 945, loss = 0.22497796\n",
      "Iteration 946, loss = 0.22486829\n",
      "Iteration 947, loss = 0.22751110\n",
      "Iteration 948, loss = 0.22809823\n",
      "Iteration 949, loss = 0.22804454\n",
      "Iteration 950, loss = 0.22441630\n",
      "Iteration 951, loss = 0.22490414\n",
      "Iteration 952, loss = 0.22389741\n",
      "Iteration 953, loss = 0.22410882\n",
      "Iteration 954, loss = 0.22343951\n",
      "Iteration 955, loss = 0.22353725\n",
      "Iteration 956, loss = 0.22375846\n",
      "Iteration 957, loss = 0.22322381\n",
      "Iteration 958, loss = 0.22412409\n",
      "Iteration 959, loss = 0.22292204\n",
      "Iteration 960, loss = 0.22266004\n",
      "Iteration 961, loss = 0.22328379\n",
      "Iteration 962, loss = 0.22211253\n",
      "Iteration 963, loss = 0.22293435\n",
      "Iteration 964, loss = 0.22243043\n",
      "Iteration 965, loss = 0.22330039\n",
      "Iteration 966, loss = 0.22208180\n",
      "Iteration 967, loss = 0.22120860\n",
      "Iteration 968, loss = 0.22142252\n",
      "Iteration 969, loss = 0.22131126\n",
      "Iteration 970, loss = 0.22106297\n",
      "Iteration 971, loss = 0.22094870\n",
      "Iteration 972, loss = 0.22084835\n",
      "Iteration 973, loss = 0.22075543\n",
      "Iteration 974, loss = 0.22080262\n",
      "Iteration 975, loss = 0.22007982\n",
      "Iteration 976, loss = 0.22002375\n",
      "Iteration 977, loss = 0.22045720\n",
      "Iteration 978, loss = 0.22164748\n",
      "Iteration 979, loss = 0.22022648\n",
      "Iteration 980, loss = 0.22673211\n",
      "Iteration 981, loss = 0.22096791\n",
      "Iteration 982, loss = 0.22315338\n",
      "Iteration 983, loss = 0.22094233\n",
      "Iteration 984, loss = 0.21993836\n",
      "Iteration 985, loss = 0.21970209\n",
      "Iteration 986, loss = 0.21818527\n",
      "Iteration 987, loss = 0.21983958\n",
      "Iteration 988, loss = 0.21841850\n",
      "Iteration 989, loss = 0.22175152\n",
      "Iteration 990, loss = 0.21975563\n",
      "Iteration 991, loss = 0.21991791\n",
      "Iteration 992, loss = 0.21740286\n",
      "Iteration 993, loss = 0.21858004\n",
      "Iteration 994, loss = 0.21824025\n",
      "Iteration 995, loss = 0.21900689\n",
      "Iteration 996, loss = 0.21669172\n",
      "Iteration 997, loss = 0.21792488\n",
      "Iteration 998, loss = 0.21591649\n",
      "Iteration 999, loss = 0.21706075\n",
      "Iteration 1000, loss = 0.21620898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAUTARO\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=5, max_iter=1000,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#activation = \"identity\"\n",
    "activation = \"logistic\"\n",
    "#activation = \"tanh\"\n",
    "#activation = \"relu\"\n",
    "mlp = MLPClassifier(#random_state=42,\n",
    "                    hidden_layer_sizes=(5),\n",
    "                    max_iter = 1000,\n",
    "                    activation = activation,\n",
    "                    verbose = True\n",
    "                    )\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de iteraciones: 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Cantidad de iteraciones: \" +str(mlp.n_iter_))\n",
    "disp = metrics.plot_confusion_matrix(mlp, X_test, y_test,cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Es posible clasificar correctamente los casos utilizando un Perceptron?\n",
    "\n",
    "Es posible clasificar los casos como se puede observar en este caso trabajamos en una capa con 5 neuronas logrando las 1000 iteraciones y arrojando en la matriz de confusion valores similares todo el tiempo\n",
    "\n",
    "- ¿Cuántas iteraciones son necesarias?\n",
    "\n",
    "Se realizaron 1000 iteraciones con un total de 5 neuronas en una capa esto puede variar durante las ejecuciones.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5a0ede1d0301589878c5ac8f8f66fa536b2434bfe9aa13bc7f7ede9900112f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
